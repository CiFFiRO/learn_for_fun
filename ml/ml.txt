машоб - раздел комп. наук, изучающий методы построения алгоритмов,
способных обучаться.

задача машоб:
	Есть обучающая выборка в которой каждый элемент описывается набором признаков x 
	для каждого которого известен ответ у.
	Требуется сконструировать функцию f(x) (решающее правило\классификатор) которая 
	выдает ответ y для любого вектора признаков x 
	Она должна "хорошо" работать на новых данных.


область работы метода = Г - все мн-во объектов на которых работает метод 
решающая ф-ция = F_0 in F - класс решающих ф-ций
Опыт = D = X x Y
целевая ф-ция = T(y, F(x)) (y - предугадываемое свойство)

векторизация - перевод представления о предмете в векторное выражение.
компоненты данного векторы называются фактором или признаком.
нормализация факторов - x` = (x-m)/sigma

бинарная классификация:
	Дана обвыборка X, где (х_i, y_i) in R^m*Y, Y={-1, 1}.
	Объекты не зависимые и взяты с определенным распределением.
	Цель- для всех новых x оценить значение argmax P(y|x).
многоклассовая классификация:
	аналогично только Y={1,...,K}
регрессия:
	Y=R и объекты взяты из неизвестного распределения и нужно оценить argmax E(y|x)
кластеризация:
	даны только векторы признаков, известно что они разбиты по кластерам на основе данной 
	метрики

формально:
	1. будем выбирать функции f из параметрического семейства F
	2. пусть L(f(x), y) - ф-ция потерь 
	3. задача обучения состоит в том, что бы найти набор параметров классификатора f, при котором 
	потери на новых данных будут минимальны 
	4. "метод классификации" = парам. семейство F + алгоритм оценки параметров по обучающей выборке 

эмпирический риск:
	пусть X^m обвыборка, тогда R_emp(f, X^m)=1/m * sum i from 1 to m of L(f(x_i), y_i)
	=> f = argmin f in F of R_emp(f, X^m)

явление переобучения:
	когда гипотеза хорошо описывает не св-ва объектов в целом, а только объектов из обучающей выборки 
	происходит когда: слишком "сложная" модель, шум в данных, плохая обвыборка 

принцип структурной минимизации риска:
	мы должны выбирать самую простую модель из достаточно точных (следствие из теории Вапника-Червоненкиса)
	пусть есть послед. F_1 subset F_2 subset .... subset F_h = F. 
	должны выбрать семейство с минимальной сложностью, но с достаточной точностью

регуляризация - метод добавления некоторых дополнительных ограничений к условию с целью решить некорректно 
поставленную задачу или предотвратить переобучение. 

принцип структурной минимизации риска:
	VC-размерность класса ф-ций F - наибольшее кол-во точек, которое мб разделено 
	ф-циями семейства, вне зависимость от конфигурации мн-ва точек.

	мы должны выбирать самую простую модель из достаточно точных (следствие из теории Вапника-Червоненкиса)
	пусть есть послед. F_1 subset F_2 subset .... subset F_h = F. 
	должны выбрать семейство с минимальной сложностью, но с достаточной точностью

удерживание:
	пусть дана обвыборка с известными ответами 
	разобьем его на две не пересекающиеся части 
	одну часть будем использовать для обучения а другую для контроля 
	=> P(f(x) != y) ~ P(f(x) != y|X_cont)

	св-ва метода:
		1. быстро и просто рассчитывается
		2. некоторые "сложные" векторы признаков могут полностью попасть только в одну из выборок 
		таким образом оценка ошибки будет смещенной 

скользящий контроль:
	разделим обвыборку на d не пресекающихся частей и поочередно будет использовать одно из
	них для контроля а оставшиеся для обучения 
	результат считается как средняя ошибка по всем итерациям 
	св-ва метода:
		1. в пределе равен общему риску 
		2. каждый вектор признаков будет один раз присутствовать в контрольной выборке 
		а так же будет в обучении
		3. некоторые "сложные" векторы признаков могут полностью попасть 
		только в один из сегментов и оценка ошибки будет смещенной

пусть есть основной класс 
ошибка 1 рода - вер-ть принять основной класс за вторичный (пропуск искомого объекта)
ошибка 2 рода - вер-ть принять вторичный класс за основной (принятие искомого объекта за "фон")
 
их важно разделять при несбалансированности классов (когда объектов одного из классов очень мало и 
других кратно больше)


Будем делить выборку на l и t подмножества - на l обучаться на t оценивать

Пусть М это метрика решения мы оптимизируем, F способы решения задачи, Т способы измерения решения
на обвыборке. Тогда оптимизируем имеем 
max T of M(argmax F of T(F, l))(t) - мы хотим получить решение F которое максимизирует М

причем M = argmax T of M(argmax F of T(F, l))(t), если параметры опт не смещены.
однако не всегда на практике формулу выше можно применить.

для построения Т будем исходить из соображений 
1. Т <=> М, усреднение М по всему доступному опыту
2. argmax F of T(F, l) = argmax F of M(F, l),
регрессия по стоимости наблюдения для приближения по Т
принцип макс энтропии
принцип мин описания
3. max T of M(argmax F of T(F, l))(t) - вер-ое моделирование получения М из удобного Т

этап 1:
F_0 = argmax F of 1/n*sum i of m(F(x_i), y_i)
тут М разбито на много м (наблюдений)
+ это естественно 
- надо следить за их независимостью 
- работает только когда нет inf 

этап 2:
A({X}) = f^-1 (1/n sum i of f(x_i)) - стоимость наблюдения 
при этом f(x) = x | log(x) | 1/x
так как они монотонные то оптимизируем:
max sum i of f(x_i)
--------------------------------------------------------------------------------------

Совместная вероятность - вер-ть одновременного наступления 
двух событий. p(x) = sum y of p(x, y)
Условная вероятность - вер-ть наступления х если у произошло
р(х, у) = р(х|у)p(y)=p(y|x)*p(x)

=> теорема Байеса p(y|x) = р(х|у)p(y)/p(x)

x и у независимые если p(x, y)=p(y)*p(x)


теорема Байеса p(theta|D) = p(theta)*p(D|theta)/p(D)
theta - параметр модели (те величины что мы обучаем)
D - данные
p(theta) - априорная вер-ть (то о чем "догадываемся"), непрерывное распред вер-ей на [0;1]
p(D|theta) - апостериорная вер-ть, правдоподобие  
p(D)=int p(D|theta)*p(theta) d theta - вер-ть данных, нормировочный коэффициент

f: a -> p(y|x=a) - ф-ция правдоподобия 

гипотеза макс. правдоподобия: theta_ML=argmax theta of p(D|theta)

p(theta|D) prop p(theta)*p(D|theta) - апостериорное распределение
макс. апост. гипотеза: theta_MAP=argmax theta of p(theta|D)=argmax theta of p(theta)*p(D|theta)
 
Байесовские предсказания:
p(x|D) = int p(x, theta | D) d theta = int p(x|theta, D) p(theta|D) d theta = так как D на x не оказывают
влияние если theta уже известно, полагаем = int p(x|theta) p(theta|D) d theta


p(f) - угадывается
p(X|f) = mul i of p(x_i|f) - считаем что точки незав. и одинаково распределенны.

F(x) = int f of p(x|f)*p(f|X) d f - решающая функция 

св-ва Байесовких методов:
плюсы:
все "честно" с точностью до данных и полученной модели 
можно использовать информацию о предыдущем обучении 
можно понять погрешность предсказания 
минусы:
сильная зависимость от выбора p(f)
сложная решающая функция
необходимо эффективно сэмплировать (вычислять F(x))

что бы пофиксить сложность вычислений можно положить самое вер-ое решение:
F(x) = f(x): p(f|X) > p(g|X) for all g in F - maximum posteriori

если нет информации о предыдущих наблюдениях и сложно придумать p(f), то 
можно положить что все решения равно вероятны:
F = argmax F in p(X|f) = argmax F in mul i of p(x_i|f) = argmax F in sum i of log(p(x_i|f))
F = argmax F in sum i of w_i * log(p(x_i|f)), если у примеров различная значимость 
--------------------------------------------------------------------------------------
РЕГРЕССИЯ

это предсказание значения 1+ вещественной переменной на основе вектора х (вектора признаков объекта)

нужно строить решающую ф-цию вида: y(x, w) + eps, где eps некоторая случ величина, может 

виды решающих ф-ций:
1. линейные: y(x, w) = w*x = sum i from 0 to D-1 of w_i*x_i, (w_0 = 1)
2. линейные базисные: y(x, w) = w*phi(x) = sum i from 0 to M-1 of w_i*phi_i(x), (phi_0 = 1)
phi_i(x) = x^i - полиноминальная регрессия 
PHI=|| phi_j(x_i) || - матрица плана

ф-ция правдоподобия:
p(t|X, w, sigma^2) = mul n from 1 to N of N(t_n|w^T*phi(x_n), sigma^2)
p(t|X, w, sigma^2) = mul n from 1 to N of 1/sqrt(2*pi*sigma^2) exp(-(t_n-w^T*phi(x_n))^2/(2*sigma^2))
ln(p(t|X, w, sigma^2)) = N*ln(1/sqrt(2*pi*sigma^2)) + sum n from 1 to N of -(t_n-w^T*phi(x_n))^2/(2*sigma^2) =
N*ln(1/sqrt(2*pi*sigma^2)) - 1/(2*sigma^2) sum n from 1 to N of (t_n-w^T*phi(x_n))^2
E(w)= 1/2 sum n from 1 to N of (t_n-w^T*phi(x_n))^2 - целевая ф-ция лин регрессии 
argmin w of E(w) - оптимальные параметры модели 
grad E(w) = -sum n from 1 to N of (t_n-w^T*phi(x_n))*phi(x_n)
grad E(w)^T = 0^T
sum n from 1 to N of (t_n-w^T*phi(x_n))*phi^T(x_n) = 0^T
t^T * PHI - w^T * PHI^T * PHI = 0^T
w^T = t^T * PHI * (PHI^T * PHI)^-1
w = (PHI^T * PHI)^-1 * PHI^T * t - оптимальные коэффициенты

однако не понятно как не эмпирически подобрать базис (степень полинома например)
решением этой проблемы является регуляризация:
E(w)= 1/2 sum n from 1 to N of (t_n-w^T*phi(x_n))^2 + lambda/2 sum j from 1 to M-1 of |w_j|^q (w_0 не регуляризуется)

при q=2

E(w)= 1/2 sum n from 1 to N of (t_n-w^T*phi(x_n))^2 + lambda/2 * w^T * w
grad E(w) = -sum n from 1 to N of (t_n-w^T*phi(x_n))*phi(x_n) + lambda * w
grad E(w)^T = 0^T
t^T * PHI - w^T * PHI^T * PHI - lambda * w^T = 0^T
w^T * (PHI^T * PHI + lambda * I) = t^T * PHI 
w^T = t^T * PHI * (PHI^T * PHI + lambda * I)^-1
w = (PHI^T * PHI + lambda * I)^-1 * PHI^T * t - оптимальные коэффициенты

вся выборка делится на три множества:
training set 60, 70% - подбор w 
validation set 20, 15% - подбор lambda, phi 
test set 20, 15% - подсчет точности 

замечание: прямое вычисление w по формулам выше адекватно если размерность матрицы плана это позволяет,
иначе используют град. спуск 
--------------------------------------------------------------------------------------
КЛАССИФИКАЦИЯ

Дана обвыборка X, где (х_i, y_i) in R^m*Y, Y={1,...,K}.
Объекты не зависимые и взяты с определенным распределением.
Цель- для всех новых x оценить значение argmax P(y|x).
Иначе построить ф-цию: y(x, w) -> {1,...,K}, x - вектор(а?) хар-ик объекта 
x = func(u), u - объект 

пусть есть основной класс 
ошибка 1 рода - вер-ть принять основной класс за вторичный (пропуск искомого объекта)
ошибка 2 рода - вер-ть принять вторичный класс за основной (принятие искомого объекта за "фон")

для бинарной классификации:
TP = |{u_i in C_1 | y(x_i, w) = 1}|
TF = |{u_i in C_0 | y(x_i, w) = 0}|
FP = |{u_i in C_0 | y(x_i, w) = 1}|
FF = |{u_i in C_1 | y(x_i, w) = 0}|

N=TP+TF+FP+FF
Accuracy=(TP+TN)/N

пусть есть основной класс 
ошибка 1 рода - вер-ть принять основной класс за вторичный (пропуск искомого объекта)
alpha = FP/(FP+TN)
ошибка 2 рода - вер-ть принять вторичный класс за основной (принятие искомого объекта за "фон")
beta = FN/(FN+TP)

чувствительность - вер-ть дать правильный ответ на пример основного класса 
избирательность - вер-ть дать правильный ответ на пример вторичного класса 

precision = TP/(TP+FP) - вер-ть что если прогнозируется принадлежность классу 1 то объект в классе 1
recall = TP/(TP+FN) - вер-ть того что если объект 1 класса то будет спрогнозируема принадлежность классу 1

F_1 = 2 * precision * recall / (precision + recall) in [0;1]

для многоклассовой классификации:
confusion matrix
C = || |{u in C_i | y(x, theta) = j}| || (KxK)

удобнее было бы иметь следующую реш функцию:
y(x, w) -> {p_1, .., p_K}, sum i of p_i = 1 и p_i >= 0

кривая precision-recall:
зависимость precision от recall 
пл-дь под ней значение качественности алгоритма 

ROC (receiver operating characteristic) кривая - отображает зависимость 1-beta от alpha 
пл-дь под кривой (area under curve AUC) - чем больше тем метод качественней 

логистическая регрессия:
нужно определить K лин регрессий, тогда будем считать что объект 
относится к t классу если z_t > z_i (i!=t)
z_i = w_i,0 + w_i^T * x
z = W*x + b (b=[w_1,0; ...; w_K,0]
t = [0, ..., 1 (t-ый), ..., 0]
y = softmax(z) = [exp(z_1)/(sum i from 1 to K of exp(z_i)); ...; exp(z_K)/(sum i from 1 to K of exp(z_i))]

p(t|x,W,b) = mul i from 1 to N of y_t_i(x_i, W, b)
y_t_i(x_i, W, b) = mul k from 1 to K of y_k(x_i, W, b)^t_i^(k)
t_i^(k) = 1 if k == t_i else 0
=> p(T|X,W,b) = mul i from 1 to N of mul k from 1 to K of y_k(x_i, W, b)^t_i^(k) -> max by W, b
-ln(p(T|X,W,b)) = -sum i from 1 to N of sum k from 1 to K of t_i^(k) * ln(y_k(x_i, W, b)) = E(W, b)
E(W, b) -> min by W,b

град спуск:
1. w_i ~ N(m=0, sigma^2=1/M) (M - размерность x) или R(-eps, eps) eps in [-1, 1]
2. (W_k, b_k) = (W_k-1, b_k-1) - gamma*(grad(E(W_k-1, b_k-1)) by W, grad(E(W_k-1, b_k-1)) by b)
3. если точность на валид. выборке не выросла конец

перед применением алгоритма нужно нормализовать данные:
x`^(j)=2*(x^(j)-min_i(x_i^(j)))/(max_i(x_i^(j))-min_i(x_i^(j)))-1 in [-1, 1]

E(W, b) = -sum k from 1 to K of t^(k) * ln(y_k(x_i, W, b)) = -ln(y_t(x_i, W, b)) =
-ln(exp(z_t)/(sum k from 1 to K of exp(z_k))) = -z_t + ln(sum k from 1 to K of exp(z_k)) =
- sum j of w_t,j*x_j - b_t + ln(sum k from 1 to K of exp( sum j of w_k,j*x_j + b_k ))

w_t,l - вес соотв. правильному классу
dE/dw_t,l = -x_l+1/(sum k from 1 to K of exp(z_k)) * exp(z_t) * x_l =
-x_l+y_t*x_l = (y_t - 1)*x_l

w_i,l (i!=t)
dE/dw_i,l = 1/(sum k from 1 to K of exp(z_k)) * exp(z_i) * x_l = y_i*x_l

grad E(W, b) by W = (y-t)*x^T

dE/db_t = -1 + 1/(sum k from 1 to K of exp(z_k)) * exp(z_t) = y_t - 1
dE/db_i = 1/(sum k from 1 to K of exp(z_k)) * exp(z_i) = y_i (i!=t)

grad E(W, b) by b = y-t

регуляризация (q=2):
E(W, b) = -sum i from 1 to N of sum k from 1 to K of t_i^(k) * ln(y_k(x_i, W, b)) + lambda/(2*N) * sum j, k of w_j,k^2

grad E(W, b) by W = (y-t)*x^T + lambda*W
grad E(W, b) by b = y-t

в алгоритме можно например усреднять градиент по всей выборке 
замечание: для оч больших выборок град считается приближенно по ее рнд части из 64\32 эл-ов (или несколько таких 
"батчей" считаются параллельно и усредняется по ним)
--------------------------------------------------------------------------------------
ДЕРЕВЬЯ РЕШЕНИЙ

дерево решений - алгоритм машоб, представляющий набор слабых классификаторов, организованных в дерево
внутренний узел дерева - это слабый классификатор,
нужно заметить что каждый внутренний узел всегда имеет как минимум двух потомков
терминальный узел - это лист дерева

слабый классификатор - недообученный классификатор, имеющий высокую ошибку, но быстро вычисляется и 
задействует малое кол-во хар-ик

разделяющая ф-ция - определяет следующий узел: h(x, theta_j): R^D x T -> {0, ..., m-1}, theta_j in T
j - номер данного внутреннего узла, theta_j - параметры h, x - обрабатываемый элемент
каждый внут узел представляет собой резделяющую функцию.

ф-ция выбора хар-ик - получает на вход вектор входных переменных х и возвращает вектор,
содержащий подмножество входных переменных вектора х.
psi(x): R^D ->R^D`, D` <= D


виды разделяющих ф-ций:
[true] = 1, [false] = 0
как правило один из tau_1, tau_2 равен +\- inf
1. h(x, theta_j) = [tau_1 > psi(x) > tau_2], theta_j = (psi, tau_1, tau_2), psi(x) = x_i 
но она очень ограниченна, в плане того что делит выборку только "гориз" и "верт" прямыми 
2. h(x, theta_j) = [tau_1 > psi(x)^T*w > tau_2], theta_j = (psi, w, tau_1, tau_2)
но она ограниченна - делит все только линейно
3. h(x, theta_j) = [tau_1 > phi(psi(x))^T*w > tau_2], theta_j = (psi, w, tau_1, tau_2)
phi - нелин фун-ция, гиперпараметр и подбирается на валид. выборке 

деревьями решений решается задача классификации 
{C_i} t from 1 to K - классы 
S_i - мн-во эл-ов обвыборки, дошедших до узла i
S_i,j - мн-во эл-ов обвыборки, дошедших до узла j, потомка i
S_i^(k) - мн-во эл-ов обвыборки класса C_k, дошедших до узла i
S_i,j^(k) - мн-во эл-ов обвыборки класса C_k, дошедших до узла j, потомка i
N_i=|S_i|
N_i^(k)=|S_i^(k)|
N_i,j=|S_i,j|
N_i,j^(k)=|S_i,j^(k)|

H(S_i) = -sum i from 1 to K of N_i^(k)/N_i * log(N_i^(k)/N_i) - энтропия узла i 
I = H(S_i) - sum j (потомок i) of N_i,j/N_i * H(S_i,j)  - разница между неопред. сейчас 
и матожид неопред. потом (прирост информации)
I -> max by theta 

терминальный узел создается если:
1. достигнута глубина d
2. N_i < n
3. H(S_i) < eps

пусть для в i-ом узле решено создать терминальный узел t
способы его задания:
t = argmax k of N_i^(k)
t = [N_i^(1)/N_i; ...; N_i^(k)/N_i]
--------------------------------------------------------------------------------------
RANDOM FOREST

ансамбль классификаторов - объединение нескольких классификаторов в один 
RANDOM FOREST - ансамбль деревьев решений 

Пусть T_1, ..., T_M - деревья решений 
задача: построить классификатор
RF(x) = RF(T_1(x), ..., T_M(x))

Пусть каждое дерево возвращает K-вектор:
T(x) = [N_i^(1)/N_i; ...; N_i^(k)/N_i]

RF(x) = 1/M * sum j from 1 to M of T_j(x)

Объединение одинаковых деревьев решений в ансамбль очевидно не даст ни какого результата

Есть два метода обучения разных деревьев:
1. Bagging 
Берется случайное подмножество обвыборки для каждого дерева, причем элементы могут повторятся
2. Random Node Optimization
лучше чем Bagging
при обучении узла вместо перебора всех параметров по всему их множеству 
мы будем перебирать их по случайному подмножеству
пусть theta_j in THETA_j
реализовывается это - в ручную по валид выборке выбираем значение r < |THETA_j| и перебираем 
r случайных вариантов theta_j

недостатки:
1. требует вектора "хороших" характеристик объектов так как не глубокий
2. все деревья учитываются одинаково, совсем плохие в том числе 
--------------------------------------------------------------------------------------
ADABOOST

решает задачу бинарной классификации 

взвешенная обвыборка - (x_i, w_i, t_i) in (X, W, T) 

для лин регрессии:
E(w)= 1/2 sum n from 1 to N of ц_т*(t_n-w^T*phi(x_n))^2
для лог регресии:
E(W, b) = -sum i from 1 to N of w_i * sum k from 1 to K of t_i^(k) * ln(y_k(x_i, W, b))
для дерева решений:
I = H_w(S_i) - sum j (потомок i) of N`_i,j/N`_i * H_w(S_i,j)
H_w(S_i) = -sum i from 1 to K of N`_i^(k)/N`_i * log(N`_i^(k)/N`_i)
N`_i = sum x_l in S_i of w_l
t_i = [N`_i^(1)/N`_i; ...; N`_i^(k)/N`_i]

постановка задачи:
C_-1, C_1, t_i in {-1, 1}, for i = 1...N
слабые классификаторы 
y_j(x) -> {-1, 1}, j=1,...,M
необходимо построить классификатор:
Y(x) = sign(sum j from 1 to M of alpha_j*y_j(x))

метод бустинг:
	1. получаем несколько классификаторов, которые верно классифицируют с вер-ю > 0.5
	2. на каждом этапе выбираем лучший классификатор, который работал лучше на примерах
	которые были "трудными" для предыдущих
	3. "трудность" это вес примера из об выборки 
	4. общий классификатор есть лин комб слабых 

на каждом шаге веса должны быть > 0 и в сумме давать 1
минимизируемая ф-ция для слабых классификаторов:
J_j = sum i of w_i^(j)*(1 if h_k(x_i) * t_i < 0 else 0) = eps_j in [0;1]
она является ошибкой классификатора 
alpha_j = 0.5 ln((1-eps_j)/eps_j)
таким образом если 
eps_j = 0.5 классификатор не учитывается 
eps_j > 0.5 классификатор инвертируется (его знак) и становиться сильнее и его ошибка пересчитывается
чем меньше ошибка тем больше вес
I(y_j(x_i) != t_i) = 1 if h_k(x_i) * y_i < 0 else 0

w_i^(j+1) = w_i^(j) * exp(alpha_i * I(y_j(x_i) != t_i))
w_i^(j+1) - если классификатор ошибся его вес "не меняется", иначе увеличивается

алгоритм AdaBoost:
	{h} - слабые классификаторы 
	y 
	(x_1, y_1), ..., (x_m, y_m) Y={-1, 1}
	1. w_1(i) - инициализация
	eps = +inf
	2. for k from 1 to K
		2.1 обучаем h_k и вычислим ошибку eps=Pr[h_k(x_i) != y_i]=sum w_k(i)*(1 if h_k(x_i) * y_i < 0 else 0)
		2.2 вес слабого классификатора alpha_k = 0.5 ln((1-eps)/eps) 
		2.3 пересчитаем веса w_k+1(i) = w_k(i) * exp(alpha_k * I(h_k(x_i) != y_i))
		2.4 нормализуем веса Z=sum i of w_k+1(i), w_k+1(i) /= Z
	3. H(x) = sign(sum k from 1 to K of alpha_k * h_k(x))

плюсы:
	1. скорость обучения 
	2. универсальность 
	3. отсутствие параметров 
	4. возможность распараллеливания 
минусы: 
	1. нужно определить нужное число итераций K 
	2. ОЧЕНЬ плохо обучается если обвыборка содержит ошибки

для борьбы с плохо сбалансированными обвыборками веса инициализируются 
w_i^(1) = 0.5 * (1/N_-1 if t_i == -1 else 1/N_1)

вывод и обоснование
E_0 = sum i from 1 to N of exp(-0.5*t_i*Y`_m(x_i))
Y`_m(x) = sum j from 1 to N of alpha_j*y_j(x)

exp(-0.5*t_i*Y`_m(x_i)) < 1 если классификатор прав 
причем если сильно уверен то -> 0
и наоборот если сильно не прав то -> +inf 


Y`_m(x) = Y`_m-1(x) + alpha_m*y_m(x)
E_0 = sum i from 1 to N of exp(-0.5*t_i*Y`_m(x_i)) = sum i from 1 to N of exp(-0.5*t_i*Y`_m-1(x_i) 
- 0.5*t_i*alpha_m*y_m(x_i)) = sum i from 1 to N of exp(-0.5*t_i*Y`_m-1(x_i)) * 
exp(- 0.5*t_i*alpha_m*y_m(x_i)) = 

w`_i^(m) = exp(-0.5*t_i*Y`_m-1(x_i))

sum i from 1 to N of w`_i^(m) * exp(- 0.5*t_i*alpha_m*y_m(x_i)) =
= Z`_m * sum i from 1 to N of exp(- 0.5*t_i*alpha_m*y_m(x_i)) -> min by alpha_m, y_m

Z`_m = sum i from 1 to N of w`_i^(m)
пусть w_i^(m) = w`_i^(m)/Z`_m => w_i^(m) > 0, sum i of w_i^(m) = 1

E = sum i from 1 to N of w_i^(m) * exp(- 0.5*t_i*alpha_m*y_m(x_i)) -> min by alpha_m, y_m
E = sum i from 1 to N of w_i^(m) * exp(- 0.5*t_i*alpha_m*y_m(x_i)) * I(y_m(x_i) != t_i) + 
sum i from 1 to N of w_i^(m) * exp(- 0.5*t_i*alpha_m*y_m(x_i)) * I(y_m(x_i) == t_i) = 

y_m(x_i) * t_i = -1 if y_m(x_i) != t_i
y_m(x_i) * t_i = 1 if y_m(x_i) == t_i

= sum i from 1 to N of w_i^(m) * exp(0.5*alpha_m) * I(y_m(x_i) != t_i) +
sum i from 1 to N of w_i^(m) * exp(-0.5*alpha_m) * I(y_m(x_i) == t_i) =
exp(0.5*alpha_m) * sum i from 1 to N of w_i^(m) * I(y_m(x_i) != t_i) +
exp(-0.5*alpha_m) * sum i from 1 to N of w_i^(m) * I(y_m(x_i) == t_i) =
= (exp(0.5*alpha_m) - exp(-0.5*alpha_m)) * sum i from 1 to N of w_i^(m) * I(y_m(x_i) != t_i) +
exp(-0.5*alpha_m) * sum i from 1 to N of w_i^(m) * [I(y_m(x_i) == t_i) + I(y_m(x_i) != t_i)] =

I(y_m(x_i) == t_i) + I(y_m(x_i) != t_i) = 1

= (exp(0.5*alpha_m) - exp(-0.5*alpha_m)) * sum i from 1 to N of w_i^(m) * I(y_m(x_i) != t_i) + 
exp(-0.5*alpha_m) -> min by y_m

J = sum i from 1 to N of w_i^(m) * I(y_m(x_i) != t_i) = eps_m -> min by y_m

E = (exp(0.5*alpha_m) - exp(-0.5*alpha_m)) * eps_m + exp(-0.5*alpha_m)
d E/d alpha_m = 0.5 * (exp(0.5*alpha_m) + exp(-0.5*alpha_m)) * eps_m - 0.5 * exp(-0.5*alpha_m) = 0
(exp(0.5*alpha_m) + exp(-0.5*alpha_m)) * eps_m - exp(-0.5*alpha_m) = 0
exp(-0.5*alpha_m) * (1 - eps_m) = exp(0.5*alpha_m) * eps_m
-0.5*alpha_m + ln(1 - eps_m) = 0.5*alpha_m + ln(eps_m)
alpha_m = ln((1 - eps_m)/eps_m)

w`_i^(m) = exp(-0.5*t_i*Y`_m-1(x_i))
w`_i^(m+1) = exp(-0.5*t_i*Y`_m(x_i)) = exp(-0.5*t_i*Y`_m-1(x_i)-0.5*alpha_m*t_i*y_m(x_i)) =
exp(-0.5*t_i*Y`_m-1(x_i))*exp(-0.5*alpha_m*t_i*y_m(x_i)) = w`_i^(m) * exp(-0.5*alpha_m*t_i*y_m(x_i)) =

заметим что t_i*y_m(x_i) = 1 - 2 * I(y_m(x_i) != t_i)

= w`_i^(m) * exp(-0.5*alpha_m + alpha_m * I(y_m(x_i) != t_i)) 

w_i^(m+1) = w`_i^(m) * exp(-0.5*alpha_m + alpha_m * I(y_m(x_i) != t_i)) / 
sum l from 1 to N w`_l^(m) * exp(-0.5*alpha_m + alpha_m * I(y_m(x_l) != t_l)) =
поделим числитель и знаменатель на Z`_m
w_i^(m) * exp(-0.5*alpha_m + alpha_m * I(y_m(x_i) != t_i)) / 
sum l from 1 to N w_l^(m) * exp(-0.5*alpha_m + alpha_m * I(y_m(x_l) != t_l)) =
w_i^(m) * exp(-0.5*alpha_m + alpha_m * I(y_m(x_i) != t_i)) / Z_m+1
--------------------------------------------------------------------------------------
КЛАСТЕРИЗАЦИЯ

это задача обучения без учителя

K-means

Пусть X - данные из R^D, C - кластеры (их K), {center_i} in R^D - центроиды

E = sum k from 1 to K of sum x in C_k ||center_k-x||^2 - целевая ф-ция
C_j = {x_i | d(x_i, center_j) = min by k of d(x_i, center_k)}

center_j^(k+1) = 1/|C_j| * sum x_i in C_j of x_i

E -> min by center 

E_k = sum x in C_k ||center_k-x||^2 =  sum x in C_k of sum i from 1 to D of (center_k,i-x_i)^2

d E_k / d center_k,i = 2 * sum x in C_k of (center_k,i-x_i) = 0
sum x in C_k of (center_k,i-x_i) = 0
center_k,i = sum x in C_k of x_i/|C_k|

=> center_k = sum x in C_k of x/|C_k|

для работы K-means нужно определения операции сложения и умножения на скаляр 

критерии останова:
1. обновление центроидов прекратилось
2. падение целевой ф-ции почти прекратилось 
3. достижение кол-ва итераций 

кластеризация К-средними:
минимизируем D(X, M) = sum k (cluster) of sum i (point in cluster_k) of ||x_i-m_k||^2
1. случайно инициализируем K центров кластеров 
2. повторяем до сходимости
2.1 назначить каждую точку ближайшему центру кластеру  
2.2 пересчитаем центр каждого кластера как среднее всех назначенных точек 

св-ва метода:
1. всего один параметр - кол-во кластеров 
2. зависит от начального приближения (иногда полезно несколько раз его применить 
и выбрать лучший вариант)
3. не учитывает строение кластеров 
4. часто применим 

Агломеративная кластеризация

пусть для каждой пары эл-ов заданно расстояние d(x,y) -> R

1 Начинаем с того, что высыпаем на каждую точку свой кластер
2 Сортируем попарные расстояния между центрами кластеров по возрастанию
3 Берём пару ближайших кластеров, склеиваем их в один и пересчитываем центр кластера
4 Повторяем п. 2 и 3 до тех пор, пока все данные не склеятся в один кластер


single-link:
d(C_i, C_j) = min by x in C_i, y in C_j of d(x, y)

avr-link:
d(C_i, C_j) = 1/(|C_i|*|C_j|) * sum x in C_i of sum y in C_j of d(x, y)

complete link:
d(C_i, C_j) = max by x in C_i, y in C_j of d(x, y)

алгоритм можно остановить на K кластерах и далее уменьшать их кол-во при необходимости 
для определения гиперпараметра K можно после каждого шага агломеративной кластеризации
вычислять целевую ф-цию E из k-means и когда она начнет падать очень полого то остановиться 




















